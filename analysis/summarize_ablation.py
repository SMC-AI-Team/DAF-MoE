"""
Ablation Study Summary
======================

Description:
    Summarizes the ablation study results (Table 5 in the paper) generated by `runner/run_ablation.py`.
    It aggregates results for different model variants (e.g., without Preservation Path)
    across specific seeds to ensure fair comparison.

    Variants analyzed:
    - Full: Complete DAF-MoE model
    - wo_raw: Without Preservation Path
    - wo_token: Without Distribution-Embedded Tokenization
    - wo_aux: Without Auxiliary Losses

Usage:
    python analysis/summarize_ablation.py
"""

import os
import json
import glob
import numpy as np
import pandas as pd

# ====================================================
# Configuration
# ====================================================
SCORE_DIR = "results/scores"       # Full Model (Baseline)
ABLATION_DIR = "results/ablation"  # Ablation Variants

# Filter: Use exactly these seeds for fair comparison
TARGET_SEEDS = [43, 44, 45, 46, 47]

# Display order for the final table
VARIANT_ORDER = [
    "Full",            
    "wo_raw",     
    # "wo_deep",
    "wo_token",        
    # "wo_spec",         
    # "wo_repel",
    "wo_aux"
]

AUPRC_KEYWORDS = ['credit', 'fraud', 'mimic']

def is_target_seed(fname):
    """Checks if the filename contains one of the target seeds."""
    for seed in TARGET_SEEDS:
        if f"seed{seed}.json" in fname:
            return True
    return False

def get_main_metric(metrics, dataset_name, config_metric=None):
    dataset_lower = dataset_name.lower()
    if config_metric and config_metric in metrics: return config_metric.upper(), metrics[config_metric]
    if 'rmse' in metrics: return 'RMSE', metrics['rmse']
    if 'auprc' in metrics and any(k in dataset_lower for k in AUPRC_KEYWORDS): return 'AUPRC', metrics['auprc']
    return 'ACC', metrics.get('acc', 0)

def parse_filename(fname):
    """Parses dataset name and variant type from the filename."""
    name_body = fname.replace(".json", "")
    if "_daf_moe_" not in name_body: return None, None
    parts = name_body.split("_daf_moe_")
    dataset_part = parts[0]
    rest_part = parts[1]
    if "_seed" in rest_part: variant_part = rest_part.split("_seed")[0]
    else: variant_part = rest_part
    return dataset_part, variant_part

def main():
    print(f"ðŸš€ Starting Fair Summary (Seeds: {TARGET_SEEDS})...")
    data_store = {}

    # ====================================================
    # 1. Load Baseline (Full Model)
    # ====================================================
    baseline_files = glob.glob(os.path.join(SCORE_DIR, "*daf_moe*.json"))
    
    # Filter by seed
    valid_baseline_files = [f for f in baseline_files if is_target_seed(os.path.basename(f))]
    
    print(f"ðŸ“‚ Loading Baseline (Full): Found {len(baseline_files)} -> Used {len(valid_baseline_files)} files")

    for fpath in valid_baseline_files:
        try:
            if "_wo_" in os.path.basename(fpath): continue # Skip ablation files in score dir

            with open(fpath, 'r', encoding='utf-8') as f: content = json.load(f)
            
            dataset = content.get('dataset')
            metrics = content.get('metrics', {})
            config = content.get('config', {})
            
            if not dataset: continue

            _, score = get_main_metric(metrics, dataset, config.get('optimize_metric'))
            
            if dataset not in data_store: data_store[dataset] = {}
            if "Full" not in data_store[dataset]: data_store[dataset]["Full"] = []
            
            data_store[dataset]["Full"].append(score)
        except Exception: continue

    # ====================================================
    # 2. Load Ablation Variants
    # ====================================================
    ablation_files = glob.glob(os.path.join(ABLATION_DIR, "*.json"))
    
    # Filter by seed
    valid_ablation_files = [f for f in ablation_files if is_target_seed(os.path.basename(f))]

    print(f"ðŸ“‚ Loading Ablation: Found {len(ablation_files)} -> Used {len(valid_ablation_files)} files")

    for fpath in valid_ablation_files:
        try:
            fname = os.path.basename(fpath)
            dataset_from_name, variant_from_name = parse_filename(fname)
            
            if not dataset_from_name or not variant_from_name: continue

            with open(fpath, 'r', encoding='utf-8') as f: content = json.load(f)
            
            dataset = content.get('dataset', dataset_from_name)
            metrics = content.get('metrics', {})
            config = content.get('config', {})

            _, score = get_main_metric(metrics, dataset, config.get('optimize_metric'))
            
            matched_variant = None
            for v_key in VARIANT_ORDER:
                if v_key == "Full": continue
                if v_key in variant_from_name:
                    matched_variant = v_key
                    break
            
            if not matched_variant: continue

            if dataset not in data_store: data_store[dataset] = {}
            if matched_variant not in data_store[dataset]: data_store[dataset][matched_variant] = []
            
            data_store[dataset][matched_variant].append(score)
        except Exception as e: print(f"âš ï¸ Error reading {fname}: {e}")

    # ====================================================
    # 3. Create DataFrame
    # ====================================================
    rows = []
    sorted_datasets = sorted(list(data_store.keys()))

    for ds in sorted_datasets:
        row = {"Dataset": ds}
        variants_data = data_store[ds]
        for var in VARIANT_ORDER:
            scores = variants_data.get(var, [])
            if len(scores) > 0:
                mean = np.mean(scores)
                std = np.std(scores)
                row[var] = f"{mean:.4f} Â± {std:.4f}"
            else:
                row[var] = "-"
        rows.append(row)

    if not rows: return

    df = pd.DataFrame(rows)
    cols = ["Dataset"] + [v for v in VARIANT_ORDER if v in df.columns]
    df = df[cols]

    print("\n" + "="*120)
    print(f"ðŸ§ª Final Ablation Summary (Exactly 5 Seeds: {TARGET_SEEDS})")
    print("="*120)
    print(df.to_string(index=False))
    print("="*120)
    
    df.to_csv("results/summarize_performance/ablation.csv", index=False)

if __name__ == "__main__":
    main()